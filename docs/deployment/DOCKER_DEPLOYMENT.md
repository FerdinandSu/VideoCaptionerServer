# VideoCaptioner Docker éƒ¨ç½²æŒ‡å—

## å‰ç½®è¦æ±‚

### 1. å®‰è£… Docker å’Œ NVIDIA Container Toolkit

**Ubuntu/Debian:**
```bash
# å®‰è£… Docker
curl -fsSL https://get.docker.com | sh

# å®‰è£… NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
    sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

**éªŒè¯ GPU è®¿é—®:**
```bash
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
```

### 2. å®‰è£… Docker Compose

```bash
# Docker Compose V2 (æ¨è)
sudo apt-get install docker-compose-plugin

# éªŒè¯å®‰è£…
docker compose version
```

## éƒ¨ç½²æ­¥éª¤

### 1. å‡†å¤‡ç›®å½•ç»“æ„

```
VideoCaptioner/
â”œâ”€â”€ settings.json           # é…ç½®æ–‡ä»¶
â”œâ”€â”€ AppData/
â”‚   â””â”€â”€ models/            # FasterWhisper æ¨¡å‹ç›®å½•
â”‚       â””â”€â”€ faster-whisper-large-v2/
â”œâ”€â”€ data/                  # æ•°æ®ç›®å½•ï¼ˆè§†é¢‘ã€å­—å¹•ç­‰ï¼‰
â”œâ”€â”€ work/                  # å·¥ä½œç›®å½•
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ main.py
```

### 2. é…ç½® settings.json

**Linux ç¯å¢ƒä¸‹çš„é…ç½®ç¤ºä¾‹:**

```json
{
  "Cache": {
    "CacheEnabled": true
  },
  "FasterWhisper": {
    "Device": "cuda",
    "Model": "large-v2",
    "ModelDir": "/app/AppData/models",
    "VadFilter": true,
    "VadMethod": "silero_v4",
    "VadThreshold": 0.4
  },
  "LLM": {
    "LLMService": "Ollama",
    "Ollama_API_Base": "http://host.docker.internal:11434/v1",
    "Ollama_API_Key": "ollama",
    "Ollama_Model": "gemma3:12b"
  },
  "RPC": {
    "Enabled": true,
    "Host": "0.0.0.0",
    "Port": 5000
  },
  "Transcribe": {
    "TranscribeLanguage": "Auto",
    "TranscribeModel": "FasterWhisper [Python] ğŸ"
  },
  "Translate": {
    "BatchSize": 10,
    "ThreadNum": 8,
    "TranslatorServiceEnum": "LLM å¤§æ¨¡å‹ç¿»è¯‘"
  },
  "Subtitle": {
    "TargetLanguage": "ç®€ä½“ä¸­æ–‡",
    "NeedTranslate": true
  }
}
```

**æ³¨æ„äº‹é¡¹:**
- `Host` è®¾ç½®ä¸º `"0.0.0.0"` ä»¥å…è®¸å®¹å™¨å¤–è®¿é—®
- `ModelDir` ä½¿ç”¨å®¹å™¨å†…è·¯å¾„ `/app/AppData/models`
- å¦‚æœè¦è®¿é—®å®¿ä¸»æœºæœåŠ¡ï¼ˆå¦‚ Ollamaï¼‰ï¼Œä½¿ç”¨ `host.docker.internal`

### 3. å‡†å¤‡æ¨¡å‹æ–‡ä»¶

å°† FasterWhisper æ¨¡å‹æ”¾åˆ° `AppData/models` ç›®å½•ï¼š

```bash
# ç¤ºä¾‹ç›®å½•ç»“æ„
AppData/models/
â”œâ”€â”€ faster-whisper-large-v2/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ model.bin
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ vocabulary.txt
```

### 4. æ„å»ºå’Œå¯åŠ¨

```bash
# æ„å»ºé•œåƒ
docker compose build

# å¯åŠ¨æœåŠ¡
docker compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker compose logs -f

# åœæ­¢æœåŠ¡
docker compose down
```

## ä½¿ç”¨æ–¹æ³•

### è®¿é—® API

**Swagger UI:**
```
http://localhost:5000/api/docs
```

**å¥åº·æ£€æŸ¥:**
```bash
curl http://localhost:5000/health
```

**å¯åŠ¨å­—å¹•åŒ–ä»»åŠ¡:**
```bash
curl -X POST http://localhost:5000/api/rpc/start-subtitize \
  -H "Content-Type: application/json" \
  -d '{
    "video_path": "/data/video.mp4",
    "raw_subtitle_path": "/data/video.srt",
    "translated_subtitle_path": "/data/video.translated.srt",
    "language": "en"
  }'
```

**æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€:**
```bash
curl http://localhost:5000/api/rpc/get-status
```

## æ•…éšœæ’æŸ¥

### 1. GPU ä¸å¯ç”¨

```bash
# æ£€æŸ¥å®¹å™¨å†… GPU
docker compose exec videocaptioner nvidia-smi

# æ£€æŸ¥ NVIDIA runtime
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
```

### 2. æŸ¥çœ‹å®¹å™¨æ—¥å¿—

```bash
# å®æ—¶æ—¥å¿—
docker compose logs -f videocaptioner

# æœ€è¿‘ 100 è¡Œ
docker compose logs --tail=100 videocaptioner
```

### 3. è¿›å…¥å®¹å™¨è°ƒè¯•

```bash
docker compose exec videocaptioner /bin/bash
```

### 4. cuDNN é—®é¢˜

å¦‚æœé‡åˆ° cuDNN é”™è¯¯ï¼Œç¡®ä¿åŸºç¡€é•œåƒç‰ˆæœ¬ä¸ NVIDIA é©±åŠ¨å…¼å®¹ï¼š

```bash
# æ£€æŸ¥é©±åŠ¨ç‰ˆæœ¬
nvidia-smi

# æ ¹æ®é©±åŠ¨é€‰æ‹©åˆé€‚çš„ CUDA é•œåƒ
# CUDA 12.x: nvidia/cuda:12.2.0-runtime-ubuntu22.04
# CUDA 11.x: nvidia/cuda:11.8.0-runtime-ubuntu22.04
```

## æ€§èƒ½ä¼˜åŒ–

### 1. æŒ‡å®š GPU

```yaml
# docker-compose.yml
environment:
  - CUDA_VISIBLE_DEVICES=0  # ä½¿ç”¨ç¬¬ä¸€å— GPU
  - CUDA_VISIBLE_DEVICES=0,1  # ä½¿ç”¨å¤šå— GPU
```

### 2. å†…å­˜é™åˆ¶

```yaml
# docker-compose.yml
deploy:
  resources:
    limits:
      memory: 8G
    reservations:
      memory: 4G
```

### 3. ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç¼“å­˜

å°† HuggingFace cache æŒ‚è½½åˆ°å®¿ä¸»æœºï¼š

```yaml
volumes:
  - ~/.cache/huggingface:/root/.cache/huggingface:rw
```

## ç”Ÿäº§ç¯å¢ƒå»ºè®®

1. **ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†æ•æ„Ÿä¿¡æ¯:**
   ```yaml
   environment:
     - OLLAMA_API_KEY=${OLLAMA_API_KEY}
   ```

2. **å¯ç”¨æ—¥å¿—è½®è½¬:**
   ```yaml
   logging:
     driver: "json-file"
     options:
       max-size: "10m"
       max-file: "3"
   ```

3. **è®¾ç½®èµ„æºé™åˆ¶:**
   ```yaml
   deploy:
     resources:
       limits:
         cpus: '4'
         memory: 8G
   ```

4. **ä½¿ç”¨åå‘ä»£ç† (Nginx):**
   ```nginx
   location /videocaptioner/ {
       proxy_pass http://localhost:5000/;
       proxy_set_header Host $host;
       proxy_set_header X-Real-IP $remote_addr;
   }
   ```

## æ›´æ–°éƒ¨ç½²

```bash
# æ‹‰å–æœ€æ–°ä»£ç 
git pull

# é‡æ–°æ„å»ºå¹¶é‡å¯
docker compose down
docker compose build
docker compose up -d
```
